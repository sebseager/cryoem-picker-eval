{"0": {
    "doc": 404,
    "title": "404",
    "content": "Page not found :( . The requested page could not be found. ",
    "url": "http://localhost:4000/404.html",
    "relUrl": "/404.html"
  },"1": {
    "doc": 404,
    "title": 404,
    "content": " ",
    "url": "http://localhost:4000/404.html",
    "relUrl": "/404.html"
  },"2": {
    "doc": "ASPIRE APPLEpicker",
    "title": "ASPIRE APPLEpicker",
    "content": " ",
    "url": "http://localhost:4000/aspire-apple.html",
    "relUrl": "/aspire-apple.html"
  },"3": {
    "doc": "ASPIRE APPLEpicker",
    "title": "Summary",
    "content": "ASPIRE (which includes APPLEpicker) is available as a set of conda-installable Python modules. We used Python 3.7 for this installation. There are several versions of APPLEpicker—a standalone Python version, a standalone MATLAB version, and an implementation within the ASPIRE project for Python. The standalone version(s) seem to have been deprecated, according to one of the collaborators here. Therefore, the following document refers only to APPLEpicker as included in the ASPIRE-Python project. The APPLEpicker paper can be found here. Refer also to the ASPIRE wiki for more information. The following guide is drawn in part from these resources. ",
    "url": "http://localhost:4000/aspire-apple.html#summary",
    "relUrl": "/aspire-apple.html#summary"
  },"4": {
    "doc": "ASPIRE APPLEpicker",
    "title": "Installation",
    "content": "ASPIRE must be installed into its own conda environment. It can be obtained either as a pip-installable package or via its git repository. Because this guide includes several patches, it will follow the latter method. First, clone the ASPIRE repo into pickers/aspire/ using . ```shell script git clone https://github.com/ComputationalCryoEM/ASPIRE-Python.git pickers/aspire . During the course of our experiments, we have created a set of patches for APPLEpicker. If you would like to apply these changes, run the patch script included in `cryo-docs/patches/aspire`. This script will replace `apple.py`, `helper.py`, and `picking.py` in the `pickers/aspire/src/aspire/apple/` directory with our patched versions. ```shell script sh patches/aspire/patch-aspire.sh pickers/aspire/ . Create a new conda environment containing the packages in the included environment.yml file. ```shell script conda env create -f pickers/aspire/environment.yml conda activate aspire . The new environment will be named `aspire` by default (as specified in the environment file), but `-n your_name_here` can be added to the `conda env create` command to change this name. The ASPIRE README recommends running their provided unit tests before installing. (Note that due to modifications made to the APPLEpicker source by our patch script above, the `testPickCenters` unit test may fail. The rest of the tests should pass with a handful of `FutureWarning`s, `DeprecationWarning`s, etc.) ```shell script (cd pickers/aspire &amp;&amp; PYTHONPATH=./src pytest tests) . In case the above does not work, a second unit test method is also provided, though it appears that it may become deprecated in future. ```shell script (cd pickers/aspire &amp;&amp; python setup.py test) . Install ASPIRE to the active conda environment (make sure you run `conda activate aspire` first, if you have not already done so). ```shell script (cd pickers/aspire &amp;&amp; python setup.py install) . ",
    "url": "http://localhost:4000/aspire-apple.html#installation",
    "relUrl": "/aspire-apple.html#installation"
  },"5": {
    "doc": "ASPIRE APPLEpicker",
    "title": "Usage",
    "content": "Overview . Inputs . | micrographs for which to pick particles | particle size, tau1, tau2, and other configurable parameters | . Outputs . | directory containing *.star coordinate files | . Setup . Configuration . [TODO: SORT OUT ASPIRE CONFIG CMD, and/or whether config.ini is read from conda package (doesn’t seem like it)] . There are several configurable parameters hard-coded in pickers/aspire/src/aspire/config.ini, including the following. [TODO: FIGURE OUT DEFINITIONS FOR EACH IF POSSIBLE] . | particle_size (default: 78) | query_image_size (default: 52) | max_particle_size (default: 156) | min_particle_size (default: 19) | tau1 (default: 710): percentage of training images believed to contain a particle | tau2 (default: 7100): percentage of training images believed may potentially contain a particle | . Picking . Start by collecting the micrograph files (*.mrc) to be picked in a directory (assuming they are not already available in their own directory). If you would like to use an existing public data set, our guide to the EMPIAR database may be helpful. ```shell script mkdir -p name_of_data_set/mrc mv path/to/your_mrc_files/*.mrc name_of_data_set/mrc . Here we will use the micrographs located in `demo_data/` as an example. Create another directory, in which any output, temporary, or configuration files will be saved by the picker. ```shell script mkdir demo_data/apple_out . To pick particles for every micrograph in demo_data/mrc/, run . ```shell script python -m aspire apple –mrc_dir demo_data/mrc/ –output_dir demo_data/apple_out/ . or, to pick a single micrograph only, run ```shell script python -m aspire apple --mrc_file demo_data/mrc/your_micrograph.mrc --output_dir demo_data/apple_out/ . The --create_jpg flag can be appended to either of the python -m aspire apple commands above to generate and save images of the micrograph(s) with particle detections overlayed. ",
    "url": "http://localhost:4000/aspire-apple.html#usage",
    "relUrl": "/aspire-apple.html#usage"
  },"6": {
    "doc": "AutoCryoPicker",
    "title": "AutoCryoPicker",
    "content": " ",
    "url": "http://localhost:4000/autocryopicker.html",
    "relUrl": "/autocryopicker.html"
  },"7": {
    "doc": "AutoCryoPicker",
    "title": "Summary",
    "content": "AutoCryoPicker is available as a GitHub Repo, as a matlab application. We used Matlab 2017b for running all commands. ",
    "url": "http://localhost:4000/autocryopicker.html#summary",
    "relUrl": "/autocryopicker.html#summary"
  },"8": {
    "doc": "AutoCryoPicker",
    "title": "Installation",
    "content": "First, clone the AutoCryoPicker repo into pickers/autocryopicker/ using . shell script git clone https://github.com/jianlin-cheng/AutoCryoPicker.git pickers/autocryopicker . In some cases, AutoCryoPicker is very slow in running, due to loading many picture files during the particle picking process. If you would like to speed it up, we have a patch under cryo-docs/patches/deeppicker to remove these pictures. This script will replace several AutoCryoPicker source files with our patched versions (keeping the others as cloned in the previous step). ",
    "url": "http://localhost:4000/autocryopicker.html#installation",
    "relUrl": "/autocryopicker.html#installation"
  },"9": {
    "doc": "DeepPicker",
    "title": "DeepPicker",
    "content": " ",
    "url": "http://localhost:4000/deeppicker.html",
    "relUrl": "/deeppicker.html"
  },"10": {
    "doc": "DeepPicker",
    "title": "Summary",
    "content": "DeepPicker is available as a GitHub repo. We used Python 3.6 for this installation. The DeepPicker paper can be found here. Refer also to the README for more information. The following guide is drawn in part from these resources. ",
    "url": "http://localhost:4000/deeppicker.html#summary",
    "relUrl": "/deeppicker.html#summary"
  },"11": {
    "doc": "DeepPicker",
    "title": "Compatibility",
    "content": "We have tested DeepPicker successfully on RHEL (Red Hat Enterprise Linux) 7.7. According to the maintainers, . it only supports Ubuntu 12.0+, centOS 7.0+, and RHEL 7.0+ . ",
    "url": "http://localhost:4000/deeppicker.html#compatibility",
    "relUrl": "/deeppicker.html#compatibility"
  },"12": {
    "doc": "DeepPicker",
    "title": "Installation",
    "content": "First, clone the DeepPicker repo into pickers/deeppicker/ using . ```shell script git clone https://github.com/nejyeah/DeepPicker-python.git pickers/deeppicker . In some cases, we have found that DeepPicker may not run properly as-is. If you would like to apply our patches to DeepPicker, run the script included in `cryo-docs/patches/deeppicker`. This script will replace several DeepPicker source files with our patched versions (keeping the others as cloned in the previous step). ```shell script sh patches/deeppicker/patch-deeppicker.sh pickers/deeppicker/ . Create and activate a new conda environment using Python version 3.6 (since tensorflow versions 1.12.0 and earlier do not support Python 3.7). ```shell script conda create -n deeppicker matplotlib scipy==1.2.1 python=3.6 conda activate deeppicker . DeepPicker requires the `tensorflow` machine learning package, which in turn requires `cudatoolkit` and `cudnn` in order to support CUDA-compatible GPUs (if your system already has global installations of CUDA and cuDNN, feel free to skip the following discussion). These can be installed manually, but it is important to use the correct version of each to avoid errors. See [this compatibility chart](https://www.tensorflow.org/install/source#gpu) for more information. The [DeepPicker GitHub](https://github.com/nejyeah/DeepPicker-python#1-install-tensorflow) indicates that `cudatoolkit` 7.5 and `cudnn` 4 should be used, but these versions are quite old and may not be compatible with modern GPU hardware (i.e., CUDA 7.5 does not support Pascal GPUs or later). To install the latest versions of `tensorflow-gpu` and all its compatible dependencies (including `cudatoolkit` and `cudnn`), use ```shell script conda install tensorflow-gpu . or, to specify a tensorflow-gpu version (which in turn will pull the correct versions of cudatoolkit and cudnn), use . ```shell script conda install tensorflow-gpu=#.##.## . Note that `conda search tensorflow-gpu` can be used to see which versions of `tensorflow-gpu` are available with conda. If the version you would like to install is not available in your conda channels, you can either specify a different channel or install everything manually with `pip`—see [release history](https://pypi.org/project/tensorflow-gpu/#history) for `tensorflow-gpu`. ## Usage [TODO: TEST THIS SECTION] ### Overview DeepPicker is a trainable particle picker, that when presented coordinate files of particles along with the micrographs of origin, can produce new `.h5` files that DeepPicker can then operate by to make refined picks. Thus, the desired application is important: - To pick by the general model that DeepPicker comes with, skip to the [Pick using pretrained model](#pick-using-pretrained-model) section. - To train a new model and then pick with that model, start by [Training a new model](#training-a-new-model), then [Pick using pretrained model](#pick-using-pretrained-model) (but substitute the `pre_trained_model` parameter with the name of the newly created model, which will be a `.h5` file). `.h5` file refers to a HDF5 format, which is a binary data format container for large arrays of data; this is an easy format for the NumPy module in python - which DeepPicker uses - to decipher. It works in DeepPicker's case to store particle information to identify. ### Training a new model #### Specifics **File formatting:** DeepPicker has a very strict format of `.star` files that it can comprehend. Our script [TODO], which converts `.box` ground truth coordinate files from EMPIAR into readable `.star` files, may be of help. For context, `.star` files are text-based file formats for storing data, essentially containing the titles and collections of data in text. **File location:** The `.star` files *in proper format* have to be in the same folder as the micrographs they correspond to. **File naming:** The `.star` files also have to have the same name as the corresponding `.mrc` file, except with an identifying suffix at the end (e.g., the micrograph `Falcon_2012_06_12-15_27_22_0.mrc` would correspond with the coordinate file `Falcon_2012_06_12-15_27_22_0_cnnPick.star`, as there is a suffix at the end; the suffix string will be identified in the script command for training for DeepPicker) #### Running training Assuming the aforementioned specifics are met, the following command will create a new model, with tailoring of parameters. Parameters outlined below as well. Note: Command is directory-specific for location of `train.py` and the inputDir; running the command below necessitates being in the same directory as `train.py`, otherwise specify the folder (folder location is specified below for from cryodocs/). ```shell script python pickers/deeppicker/train.py --train_type 1 --train_inputDir \"input_dir\" --particle_size 160 --mrc_number -1 --particle_number -1 --coordinate_symbol 'some_string' --model_save_dir 'output_dir' --model_save_file 'output_model_name' . Parameters . | --train_inputDir: input directory of .star and corresponding .mrc files | --train_type: options are 1, 2, 3, or 4. 1 is recommended, for specimen-specific new models; 2 for multiple molecules, 3 for iterative training. | --mrc_number: number of .mrc files to pick from the directory specified; default=-1 refers to all | --particle_size: the size of the particle | --coordinate_symbol: suffix that identifies .star file for each .mrc file; refer [Specifics] | --model_save_dir: the directory to save the model .h5 file to | --model_save_file: the name of the model .h5 file | . Pick using pretrained model . Start by collecting the micrograph files (*.mrc) to be picked in a directory (assuming they are not already available in their own directory). If you would like to use an existing public data set, our guide to the EMPIAR database may be helpful. ```shell script mkdir -p name_of_data_set/mrc mv path/to/your_mrc_files/*.mrc name_of_data_set/mrc . Here we will use the micrographs located in `demo_data/` as an example. Create another directory, in which any output, temporary, or configuration files will be saved by the picker. ```shell script mkdir demo_data/deeppicker_out . Use the following command to pick all micrographs in demo_data/mrc/. A description of parameters is given below. Note: Command is again directory-specific for autoPick.py; you need to be in the same directory as autoPick.py or specify the folder location (folder location is specified below for from cryodocs/). shell script python pickers/deeppicker/autoPick.py --inputDir 'demo_data/mrc/' --pre_trained_model 'pretrained_or_created_model' --particle_size 176 --mrc_number -1 --outputDir 'demo_data/deepicker_out' --coordinate_symbol '_dp' --threshold 0.5 . Parameters . | --inputDir: input directory of .mrc files | --pre_trained_model: the .h5 model file | --mrc_number: number of .mrc files to pick from the directory specified; default=-1 refers to all | --particle_size: the size of the particle | --outputDir: output directory to save the coordinate .star files | --coordinate_symbol: suffix to be appended to the filenames of the output coordinate files (e.g., the input micrograph Falcon_2012_06_12-15_27_22_0.mrc would correspond with an output coordinate file named Falcon_2012_06_12-15_27_22_0_cnnPick.star) | . ",
    "url": "http://localhost:4000/deeppicker.html#installation",
    "relUrl": "/deeppicker.html#installation"
  },"13": {
    "doc": "Home",
    "title": "Cryo-EM Particle Picker Docs",
    "content": " ",
    "url": "http://localhost:4000/#cryo-em-particle-picker-docs",
    "relUrl": "/#cryo-em-particle-picker-docs"
  },"14": {
    "doc": "Home",
    "title": "Table of Contents",
    "content": ". | Introduction (this document) | Particle picker guides | Miscellaneous . | EMPIAR usage guide | . | . ",
    "url": "http://localhost:4000/#table-of-contents",
    "relUrl": "/#table-of-contents"
  },"15": {
    "doc": "Home",
    "title": "Introduction",
    "content": "This repository (repo) contains installation guides, procedures, and patches for cryo-EM particle pickers, as followed during our experimentation for [TODO: FILL PAPER CITATION]. These guides are primarily written for use on Linux systems; it may be possible to adapt some procedures to other *NIX systems like macOS with relatively minor changes, but we have not tested these use cases. Installations on Windows will likely be more complicated, and may require various supplementary procedures or compatibility layers. We recommend something like the following directory hierarchy for ease of housekeeping, and will assume this general structure from now on. cryo-docs/ &lt;-- this repo ├── docs/ &lt;-- picker installation/usage guides ├── patches/ ├── pickers/ │ ├── picker1/ ⎤ │ ├── picker2/ ⎥ pickers to be installed here │ └── picker3/ ⎦ ├── demo_data/ │ ├── mrc/ │ ├── train_mrc/ │ ├── train_coord/ │ ├── picker1_out/ ⎤ │ ├── picker2_out/ ⎥ picker output, created in guides │ └── picker3_out/ ⎦ ... etc. If you haven’t already, please clone the most recent version of this repo (which follows the above structure) and change directory into the clone. All installation guides will begin by assuming that your current working directory is cryo-docs/. Note that here and in all guides, /path/to/something should be replaced by the path to the indicated resource on your system. ```shell script git clone –depth 1 https://github.com/sebseager/cryo-docs.git &amp;&amp; cd cryo-docs . This repo's `demo_data/` directory contains some sample data (five micrographs from the [EMPIAR-10017](https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10017/) beta-galactosidase data set) that can be used to test picker installations. All guides will refer to these data, but any references to `demo_data/` can be replaced with applicable paths to your own micrograph data. To get started, take a look at our particle picker [usage guides](/docs) (or navigate using the table of contents above). ## Additional remarks ### Anaconda/Miniconda Most of these guides assume that the Anaconda package manager (referred to as conda) is available on your system. If not, it can be installed by following [these instructions](https://docs.conda.io/projects/conda/en/latest/user-guide/install/). We recommend using the lighter Miniconda distribution for most applications. A PDF with a list of useful conda commands can be found [here](https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf). When switching between different conda environments, we will use `conda activate my_environment_name` and `conda deactivate my_environment_name`. This is the preferred method to use with conda versions 4.4.0 and above. A possible exception to this may be shell scripts or cluster job queue scripts, where you may find that the following method works better: ```shell script source /path/to/miniconda#/bin/activate /path/to/conda_envs/my_environment_name . If the conda activate or conda deactive commands do not work, you will need to add the conda executable to your PATH environment variable. To allow conda to run its built-in setup automatically, use . ```shell script /path/to/miniconda#/bin/conda init . or, to add conda to your `PATH` temporarily (for your current shell session only), use ```shell script export PATH=/path/to/miniconda#/bin:$PATH . or, to add conda to your PATH such that it persists across shell sessions, add it to your ~/.bashrc (or ~/.zshrc, etc. depending on your shell) like so . shell script echo 'export PATH=/path/to/miniconda#/bin:$PATH' &gt;&gt; ~/.bashrc . ",
    "url": "http://localhost:4000/#introduction",
    "relUrl": "/#introduction"
  },"16": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"17": {
    "doc": "PARSED",
    "title": "PARSED",
    "content": " ",
    "url": "http://localhost:4000/parsed.html",
    "relUrl": "/parsed.html"
  },"18": {
    "doc": "PARSED",
    "title": "Summary",
    "content": "PARSED is available as a collection of Python programs. We used Python 3.6 for this installation. The PARSED paper can be found here. Both the PARSED package files and a user manual can be downloaded in the parsed_v1.zip archive available with the supplementary materials of the PARSED paper. We include all relevant files in pickers/parsed/ of this repo for ease of access. In doing so, we adopt for these files the same usage license identified in the paper, namely: . the PARSED package and user manual [are available] for noncommercial use . ",
    "url": "http://localhost:4000/parsed.html#summary",
    "relUrl": "/parsed.html#summary"
  },"19": {
    "doc": "PARSED",
    "title": "Installation",
    "content": "First, make a new directory for the PARSED package. ```shell script mkdir parsed . PARSED is available as it was originally published in `pickers/parsed/original/`. Modified versions of these files with our patches and bug fixes are available in in `pickers/parsed/`. We will assume the patched files are being used, but the original software can be used at any time by replacing references to `pickers/parsed/` with `pickers/parsed/original` where applicable. Create and activate a new conda environment with the required dependencies. It may take conda several tries to solve this environment due to the highly specific package dependencies. The versions specified for each package correspond with those listed on the first page of the [PARSED user manual](/pickers/parsed/PARSED_Manual_V1.pdf). It may be possible to use later versions of some packages (e.g., `mrcfile`) without adverse effects. ```shell script conda create -n parsed -c conda-forge h5py=2.7.1 keras=2.0.8 numba=0.37.0 pandas=0.20.3 matplotlib=2.1.0 mrcfile=1.1.2 trackpy=0.4.1 tensorflow-gpu=1.7.0 pip python=3.6 conda activate parsed . Install any additional dependencies (not directly available through conda) with pip. In doing so, however, it is important that we use the pip executable that was just installed in the conda environment created above. To verify this, check that running which pip outputs something like /path/to/conda_envs/parsed/bin/pip. If not, try restarting your shell session or running conda deactivate parsed followed by conda activate parsed before proceeding. ```shell script pip install opencv-python==3.1.* . ## Usage ### Overview Inputs - micrographs for which to pick particles - picking aperture size, micrograph pixel resolution, and other configurable parameters - general model `*.h5` file Outputs (in a single directory) - `*.star` coordinate files (for context, `.star` files are text-based file formats for storing data, essentially containing the titles and collections of data in text) - `*.star` extended parameter files (containing coordinates, but also blob mass and other data) - `*.h5` data files (not used) ### Pick using pretrained model Start by collecting the micrograph files (`*.mrc`) to be picked in a directory (assuming they are not already available in their own directory). If you would like to use an existing public data set, [our guide to the EMPIAR database](/empiar.html) may be helpful. ```shell script mkdir -p name_of_data_set/mrc mv path/to/your_mrc_files/*.mrc name_of_data_set/mrc . Here we will use the micrographs located in demo_data/ as an example. Create another directory, in which any output, temporary, or configuration files will be saved by the picker. ```shell script mkdir demo_data/parsed_out . Use the following command to pick all micrographs in `demo_data/mrc/`. A description of parameters (modified from the [PARSED user manual](/pickers/parsed/PARSED_Manual_V1.pdf)) is given below. ```shell script python -W ignore parsed_main.py --model=pickers/parsed/pre_train_model.h5 --data_path=demo_data/mrc/ --output_path=demo_data/parsed_out/ --file_pattern=*.mrc --job_suffix=autopick --angpixel=1.34 --img_size=4096 --edge_cut=0 --core_num=4 --aperture=160 --mass_min=4 . When providing arguments for the command above, relative directories can be specified with ./ and ../, but do not use ~, as it does not appear to expand to the user’s home folder. Note that if PARSED does not recognize a path, it may exit with the usual No such file or directory, or it may also attempt to pick anyway, returning Coordinates extraction finished 0 TotalPickNum found. Parameters . | --model: pre-trained segmentation model | --data_path: path to directory of input micrographs | --output_path: path to output directory | --file_pattern: regex (regular expression)-like pattern that describes the filenames of the micrographs to be picked (e.g., *.mrc includes only files ending in .mrc) | --job_suffix: suffix to be appended to the filenames of the output coordinate files (e.g., the input micrograph Falcon_2012_06_12-15_27_22_0.mrc would correspond with an output coordinate file named Falcon_2012_06_12-15_27_22_0_autopick.star) | --angpixel: sample rate (pixel resolution) for specified micrographs (in Å/pixel) | --img_size: either the width or height of the micrograph in pixels—whichever is larger | --edge_cut: number of pixels to “crop” from each edge of the micrograph | --core_num: number of processes to run in parallel | --aperture: diameter of the particle-picking aperture (in Å) | --mass_min: minimal picking mass of detected blobs (particle candidates) | . Note: a GPU ID to use for picking can also be specified by appending the --gpu_id # flag, where # is the GPU ID. On an NVIDIA-based system, use the nvidia-smi command to get a list of available GPUs and their IDs. ",
    "url": "http://localhost:4000/parsed.html#installation",
    "relUrl": "/parsed.html#installation"
  },"20": {
    "doc": "Particle Pickers",
    "title": "Particle Pickers",
    "content": " ",
    "url": "http://localhost:4000/pickers",
    "relUrl": "/pickers"
  },"21": {
    "doc": "RELION LoG (Laplacian-of-Gaussian) Picker",
    "title": "RELION LoG (Laplacian-of-Gaussian) Picker",
    "content": " ",
    "url": "http://localhost:4000/relion-log.html",
    "relUrl": "/relion-log.html"
  },"22": {
    "doc": "RELION LoG (Laplacian-of-Gaussian) Picker",
    "title": "Summary and Installation",
    "content": "The LoG auto-picker is available as part of the RELION package. This guide assumes that RELION is already installed on your system or computing cluster. If not, please contact your systems administrator. More information can be found in the RELION paper, GitHub repo, and in particular the RELION 3.0 tutorial document. ",
    "url": "http://localhost:4000/relion-log.html#summary-and-installation",
    "relUrl": "/relion-log.html#summary-and-installation"
  },"23": {
    "doc": "RELION LoG (Laplacian-of-Gaussian) Picker",
    "title": "Usage",
    "content": "Section 1.5 of the tutorial document linked above discusses the LoG picker. ",
    "url": "http://localhost:4000/relion-log.html#usage",
    "relUrl": "/relion-log.html#usage"
  },"24": {
    "doc": "SPHIRE-crYOLO",
    "title": "SPHIRE-crYOLO",
    "content": " ",
    "url": "http://localhost:4000/sphire-cryolo.html",
    "relUrl": "/sphire-cryolo.html"
  },"25": {
    "doc": "SPHIRE-crYOLO",
    "title": "Summary",
    "content": "crYOLO is available as a Python package from PyPI. We used Python 3.6 for this installation. The crYOLO paper can be found here. Refer also to the wiki (potentially outdated) and the readthedocs user guide for more information. The following guide is drawn in part from these resources. ",
    "url": "http://localhost:4000/sphire-cryolo.html#summary",
    "relUrl": "/sphire-cryolo.html#summary"
  },"26": {
    "doc": "SPHIRE-crYOLO",
    "title": "Installation",
    "content": "Create and activate a new conda environment with the required dependencies. ```shell script conda create -n cryolo -c conda-forge -c anaconda python=3.6 pyqt=5 cudnn=7.1.2 numpy==1.14.5 cython wxPython==4.0.4 intel-openmp==2019.4 pip conda activate cryolo . Since crYOLO is available through PyPI, it can be installed using the package manager `pip`. In doing so, however, it is important that we use the `pip` executable that was just installed in the conda environment created above. To verify this, check that running `which pip` outputs something like `/path/to/conda_envs/cryolo/bin/pip`, and that `which python` outputs something like `/path/to/conda_envs/cryolo/bin/python`. To install crYOLO with GPU support (recommended, if you have a GPU available), run ```shell script pip install 'cryolo[gpu]' . or, to install crYOLO with CPU support only, run . ```shell script pip install ‘cryolo[cpu]’ . ### General models crYOLO provides three general models, each of which has been trained on a variety of data sets. They are available in [this section](https://cryolo.readthedocs.io/en/latest/installation.html#download-the-general-models) of the user guide. The following commands can also be used to download the most current versions (as of October 1, 2020) to your current working directory. Low-pass filtered: ```shell script wget ftp://ftp.gwdg.de/pub/misc/sphire/crYOLO-GENERAL-MODELS/gmodel_phosnet_202005_N63_c17.h5 . Neural-network (JANNI) denoised: . ```shell script wget ftp://ftp.gwdg.de/pub/misc/sphire/crYOLO-GENERAL-MODELS/gmodel_phosnet_202005_nn_N63_c17.h5 . Negative stain: ```shell script wget ftp://ftp.gwdg.de/pub/misc/sphire/crYOLO-GENERAL-MODELS/gmodel_phosnet_negstain_20190226.h5 . ",
    "url": "http://localhost:4000/sphire-cryolo.html#installation",
    "relUrl": "/sphire-cryolo.html#installation"
  },"27": {
    "doc": "SPHIRE-crYOLO",
    "title": "Usage",
    "content": "Overview . Inputs . | micrographs for which to pick particles | particle box size and other configurable parameters | manually picked particle coordinate files for training set (optional) | general model *.h5 file (optional unless using JANNI denoising or performing model training) | . Outputs . | directory containing *.box coordinate files | directory containing *.star coordinate files | directory containing *.cbox coordinate files | directory containing confidence distributions and other statistical data | . Setup . Start by collecting the micrograph files (*.mrc) to be picked in a directory (assuming they are not already available in their own directory). If you would like to use an existing public data set, our guide to the EMPIAR database may be helpful. ```shell script mkdir -p name_of_data_set/mrc mv path/to/your_mrc_files/*.mrc name_of_data_set/mrc . Here we will use the micrographs located in `demo_data/` as an example. Create another directory, in which any output, temporary, or configuration files will be saved by the picker. ```shell script mkdir demo_data/cryolo_out . This guide covers usage of crYOLO from the command line. A crYOLO GUI (see this tutorial) is also available, which provides approximately the same functionality as the command line interface. The GUI requires either a physical monitor connected to the machine running crYOLO or an X11 display forwarding configuration (which sends the GUI to your machine over SSH). The GUI can be accessed by running cryolo_gui.py with no arguments. The remainder of this guide will refer to the command line interface, but users of the GUI may be able to follow along (e.g., the cryolo_predict.py command is represented graphically by the predict action in the sidebar). Box size determination . According to EMAN2 box size standards, . the particle box-size must be 1.5-2x the size of the largest axis of your particle . and, for optimal performance, should be selected from the following list: . 16, 24, 32, 36, 40, 44, 48, 52, 56, 60, 64, 72, 84, 96, 100, 104, 112, 120, 128, 132, 140, 168, 180, 192, 196, 208, 216, 220, 224, 240, 256, 260, 288, 300, 320, 352, 360, 384, 416, 440, 448, 480, 512, 540, 560, 576, 588, 600, 630, 640, 648, 672, 686, 700, 720, 750, 756, 768, 784, 800, 810, 840, 864, 882, 896, 900, 960, 972, 980, 1000, 1008, 1024 . Creating training data . Note: crYOLO can pick particles using its general model alone. If you would like to do this, feel free to skip this section and continue to configuration method 1. However, it is also possible to refine that model or train a new one from scratch to fit your data (see configuration methods 2 and 3 below). This section describes how training data can be created. In order to isolate training data, a subset of the micrographs (here, some train_1.mrc, train_2.mrc, etc. in demo_data/mrc/) should be placed into a separate directory (demo_data/train_mrc/). These images, along with some known particle coordinates for each (in demo_data/train_coord/), will be used to train the model. crYOLO matches an image to its corresponding coordinate file by comparing the filenames (e.g. demo_data/train_mrc/Falcon_2012_06_12-14_57_34_0.mrc and demo_data/train_coord/Falcon_2012_06_12-14_57_34_0.box would be paired). ```shell script mkdir demo_data/train_mrc demo_data/train_coord mv demo_data/mrc/{train_1.mrc,train_2.mrc,train_3.mrc} demo_data/train_mrc/ . To populate `train_coord/`, the provided `cryolo_boxmanager.py` or other tools like `e2boxer` (see EMAN2 wiki [here](https://blake.bcm.edu/emanwiki/EMAN2/Programs/e2boxer)) can be used. They provide a graphical interface for manually selecting particle coordinates. Note that currently `cryolo_boxmanager.py` does not support filaments, so `e2helixboxer.py` is recommended in that case (more on that [here](https://cryolo.readthedocs.io/en/stable/tutorials/tutorial_overview.html#id6)). To open a micrograph for manual picking in the crYOLO box manager, run `cryolo_boxmanager.py`, click `File` → `Open image folder`, and select the `train_mrc/` directory created and populated above. From here, there are options to apply a temporary low-pass filter or change the box size (see the [Box size determination](#box-size-determination) section above). Within a manual picking window, - Left-click a particle to select it - Left-click-and-drag to move a box - Use `Ctrl` + left-click to delete a box To save your picks, select `File` → `Write box files`. ### Configuration Configure crYOLO by choosing a configuration file location, box size (220 in this example), and image filter. Use one of the following commands to generate picking configuration files, depending on the data set you are working with, the kind of model you would like to use, and whether or not you would like to train your own model. Note that for each of the `config` commands given below, other options are usually available (run `cryolo_gui.py config --help` for a good description of each). Most will probably not need to be changed. #### Method 1. Use pretrained general model as-is The crYOLO [general model](https://cryolo.readthedocs.io/en/latest/other/other.html#general-model-data-sets) is trained on the following EMPIAR data sets: &gt; 10023, 10004, 10017, 10025, 10028, 10050, 10072, 10081, 10154, 10181, 10186, 10160, 10033, 10097 as well as simulated data sets based on the following PDB entries: &gt; 1sa0, 5lnk, 5xnl, 6b7n, 6bhu, 6dmr, 6ds5, 6gdg, 6h3n, 6mpu Use one of these commands to generate a configuration file. | Model | Sample command |---------------------|-------------------------------------------------------------------------------------------------------------------------| Phosaurus low-pass* | `cryolo_gui.py config demo_data/cryolo_out/cryolo_config.json 220 --filter LOWPASS --low_pass_cutoff 0.1` | JANNI-denoised** | `cryolo_gui.py config demo_data/cryolo_out/cryolo_config.json 220 --filter JANNI --janni_model /path/to/janni_model.h5` | Negative stain | `cryolo_gui.py config demo_data/cryolo_out/cryolo_config.json 220 --filter NONE` | *\\* The* `low_pass_cutoff` *can be changed to anything between 0 and 0.5 inclusive, but 0.1 is the default* *\\*\\* The* `janni_model.h5` *referenced here can be downloaded at the end of the [Installation](#installation) section above.* #### Method 2. Refine the general model to your data *We assume that you have a directory* `demo_data/train_mrc/` *containing a set of training micrographs, and another* `demo_data/train_coord/` *containing corresponding coordinate files. If not, please take a look at the [Creating training data](#creating-training-data) section above.* To allow crYOLO to separate automatically a random 20% of the training set for use as a validation set (default), run ```shell script cryolo_gui.py config demo_data/cryolo_out/cryolo_config.json 220 --train_image_folder demo_data/train_mrc/ --train_annot_folder demo_data/train_coord/ --pretrained_weights /path/to/one_of_the_gmodels.h5 . Otherwise, to specify validation images and their corresponding coordinate files, make new directories demo_data/valid_mrc/ and demo_data/valid_coord/, populate them with micrographs and box files accordingly, and run the configuration command. ```shell script mkdir demo_data/valid_mrc demo_data/valid_coord mv demo_data/train_mrc/{train_1.mrc,train_2.mrc} demo_data/valid_mrc/ mv demo_data/train_coord/{train_1.box,train_2.box} demo_data/valid_coord/ cryolo_gui.py config demo_data/cryolo_out/cryolo_config.json 220 –train_image_folder demo_data/train_mrc/ –train_annot_folder demo_data/train_coord/ –pretrained_weights /path/to/one_of_the_gmodels.h5 –valid_image_folder demo_data/valid_mrc/ –valid_annot_folder demo_data/valid_coord/ . #### Method 3. Train your own model from scratch *We assume that you have a directory* `demo_data/train_mrc/` *containing a set of training micrographs, and another* `demo_data/train_coord/` *containing corresponding coordinate files. If not, please take a look at the [Creating training data](#creating-training-data) section above.* To allow crYOLO to separate automatically a random 20% of the training set for use as a validation set (default), run ```shell script cryolo_gui.py config demo_data/cryolo_out/cryolo_config.json 220 --train_image_folder demo_data/train_mrc/ --train_annot_folder demo_data/train_coord/ . Otherwise, to specify validation images and their corresponding coordinate files, make new directories demo_data/valid_mrc/ and demo_data/valid_coord/, populate them with micrographs and box files accordingly, and run the configuration command. ```shell script mkdir demo_data/valid_mrc demo_data/valid_coord mv demo_data/train_mrc/{train_1.mrc,train_2.mrc} demo_data/valid_mrc/ mv demo_data/train_coord/{train_1.box,train_2.box} demo_data/valid_coord/ cryolo_gui.py config demo_data/cryolo_out/cryolo_config.json 220 –train_image_folder demo_data/train_mrc/ –train_annot_folder demo_data/train_coord/ –valid_image_folder demo_data/valid_mrc/ –valid_annot_folder demo_data/valid_coord/ . ### Training *Note: this section only applies to configuration methods [2](#method-2-refine-the-general-model-to-your-data) and [3](#method-3-train-your-own-model-from-scratch) above.* Use a command below according to your intended training method. Note that it is possible to run method 3 below (training from scratch) using configuration method [2](#method-2-refine-the-general-model-to-your-data) above. This allows the new model's weights to be initialized closer (potentially) to the values they ought to end up at, while still performing \"from scratch\" training (*not* refinement). | Method | Sample command |-----------------------------|-------------------------------------------------------------------------------------------| 1. General model as-is | N/A (no training required) | 2. General model refinement | `cryolo_train.py -c demo_data/cryolo_out/cryolo_config.json -w 0 -g 0 --fine_tune -lft 2` | 3. Training from scratch | `cryolo_train.py -c demo_data/cryolo_out/cryolo_config.json -w 5 -g 0` | The `-w` flag sets the number of warmup epochs, and must be zero when using `--fine_tune` (for model refinement). The `-lft` flag sets the number of layers to fine tune, for which the authors recommend a default of 2. The `-g` flag indicates the GPU ID(s) to be used in training. On an NVIDIA-based system, use the `nvidia-smi` command to get a list of available GPUs and their IDs. Specify multiple GPUs with something like `-g '0 1 2'`. You might also consider adding the `--cleanup` flag, which deletes filtered images after training, if you would like to conserve storage space on your system. ### Picking To pick particles (not filaments) for every micrograph in `mrc/` using either one of the general models (if following configuration/training method 1) or a model you refined or trained in the previous section, run the following: ```shell script cryolo_predict.py -c demo_data/cryolo_out/cryolo_config.json -w path/to/model.h5 -i mrc/ -g 0 -o demo_data/cryolo_out/ -t 0.3 . The flag -t sets the confidence threshold (i.e., how “sure” crYOLO is that a particular detection is actually a particle) below which picks will not be included in the output *.star and *.box coordinate files. Regardless of this value, however, all picks—along with their confidence values—will be recorded in demo_data/cryolo_out/CBOX/*.cbox files. If picking filaments, the following command can be used: . shell script cryolo_predict.py -c demo_data/cryolo_out/cryolo_config.json -w path/to/model.h5 -i mrc/ -g 0 -o demo_data/cryolo_out/ -t 0.3 --filament -fw 100 -bd 20 -mn 6 . where the -fw indicates filament width in pixels, -bd indicates the distance between adjacent boxes on the filament, and -mn indicates the smallest number of boxes that are allowed to constitute a filament. ",
    "url": "http://localhost:4000/sphire-cryolo.html#usage",
    "relUrl": "/sphire-cryolo.html#usage"
  },"28": {
    "doc": "Topaz",
    "title": "Topaz",
    "content": " ",
    "url": "http://localhost:4000/topaz.html",
    "relUrl": "/topaz.html"
  },"29": {
    "doc": "Topaz",
    "title": "Summary",
    "content": "Topaz is available as a conda package. We used Python 3.6 for this installation. The Topaz paper can be found here. Refer also to the GitHub repo, the web-based GUI for generating Topaz commands, and the tutorials included in the Topaz repo for more information. The following guide is drawn in part from these resources. ",
    "url": "http://localhost:4000/topaz.html#summary",
    "relUrl": "/topaz.html#summary"
  },"30": {
    "doc": "Topaz",
    "title": "Installation",
    "content": "Create and activate a new conda environment (named topaz) with the required dependencies. ```shell script conda create -n topaz python=3.6 conda activate topaz . With the `topaz` environment active, install the Topaz package with its dependencies. ```shell script conda install topaz -c tbepler -c pytorch . If your machine/computing cluster does not already have a global installation of cudatoolkit available, it can be installed to the topaz environment, as follows. ```shell script conda install cudatoolkit=9.0 -c pytorch . Verify that the Topaz installation works by running `topaz --help`. If it does, you should see a help menu. If your shell returns something like `topaz: command not found`, you may need to deactivate and reactivate the environment. ```shell script conda deactivate topaz conda activate topaz . If you see a traceback (error), however, you may also need to install the future package. Make sure you’re in the topaz environment before doing so. ```shell script conda install future . ## Usage The authors of Topaz provide [several detailed tutorials](https://github.com/tbepler/topaz/tree/master/tutorial) in their repository—in particular, in their [quick start guide](https://github.com/tbepler/topaz/blob/master/tutorial/01_quick_start_guide.ipynb) and [more detailed walkthrough](https://github.com/tbepler/topaz/blob/master/tutorial/02_walkthrough.ipynb). The following outlines broader usage examples and practices. Start by collecting the micrograph files (`*.mrc`) to be picked in a directory (assuming they are not already available in their own directory). If you would like to use an existing public data set, [our guide to the EMPIAR database](/empiar.html) may be helpful. ```shell script mkdir -p name_of_data_set/mrc mv path/to/your_mrc_files/*.mrc name_of_data_set/mrc . Here we will use the micrographs located in demo_data/ as an example. Preprocessing . Create a Topaz output directory with subdirectories for micrograph preprocessing. ```shell script mkdir -p demo_data/topaz_out/processed/micrographs . We will now process the micrographs with the `preprocess` command (which combines `downsample` and `normalize` operations). This will downscale the micrographs by a specified multiplier, which is intended to help the neural network learn and converge more quickly. It is suggested in the [detailed walkthrough](https://github.com/tbepler/topaz/blob/master/tutorial/02_walkthrough.ipynb) that a downsampling multiplier should be chosen as follows: &gt; We recommend downsampling your data enough that the diameter of your particle fits within the receptive field of the CNN architecture you are using ... as a rule of thumb, downsampling to about 4-8 Å per pixel generally works well, but this may need to be adjusted for very large or very small particles to fit the classifier For reference, the training tab of the [Topaz GUI](https://emgweb.nysbc.org/topaz.html) provides the following classifier specifications: &gt; Your particle must have a diameter (longest dimension) after downsampling of: &gt; - 70 pixels or less for resnet8 &gt; - 30 pixels or less for conv31 &gt; - 62 pixels or less for conv63 &gt; - 126 pixels or less for conv127 For example, if the original micrograph resolution was 1.2 Å/pix, a downsampling factor of 5 would bring the preprocessed micrograph's resolution to 6 Å/pix. The preprocessing command would be as follows. ```shell script topaz preprocess -s 6 -o demo_data/topaz_out/processed/micrographs/ demo_data/mrc/*.mrc . Topaz has two primary picking strategies: using the pretrained general model, or training a new model (optionally initialized with pretrained weights). Method 1: Use pretrained general model as-is . The extract command takes input and output paths, as well two numerical parameters. The -r parameter should be set to the radius of the particle you would like to pick. It is recommended that this be kept relatively small (as appropriate for your particle), as Topaz will not pick particles any closer than this to prevent multiple detections per particle. The -x parameter will upscale the resulting picks to the original micrograph, and should be the same as -s from topaz preprocess. ```shell script topaz extract -r 14 -x 6 -o demo_data/topaz_out/predicted_particles_all_upsampled.txt demo_data/topaz_out/processed/micrographs/*.mrc . ### Method 2: Train a model from scratch In order to isolate training data, a subset of the micrographs (here, some `train_1.mrc`, `train_2.mrc`, etc. in `demo_data/mrc/`) should be placed into a separate directory (`demo_data/train_mrc/`). These images, along with some known particle coordinates for each (in `demo_data/train_coord/`), will be used to train the model. crYOLO matches an image to its corresponding coordinate file by comparing the filenames (e.g. `demo_data/train_mrc/Falcon_2012_06_12-14_57_34_0.mrc` and `demo_data/train_coord/Falcon_2012_06_12-14_57_34_0.star` would be paired). ```shell script mkdir demo_data/train_mrc demo_data/train_coord mv demo_data/mrc/{train_1.mrc,train_2.mrc,train_3.mrc} demo_data/train_mrc/ . To populate train_coord/, software like EMAN2’s e2boxer may be used to generate coordinate files for the training micrographs. Topaz also provides an online graphical interface (located here, in the Pick | Analyze tab) which can be used to generate training coordinates. For the sake of example, the .star files located in demo_data/star can be used. Note that Topaz takes training coordinates as a single file of the following format (columns are separated by single \\t tab characters): . image_name x_coord y_coord Falcon_2012_06_12-14_57_34_0 3822 3477 Falcon_2012_06_12-14_57_34_0 3810 3402 ... Topaz also supports conversion from other file formats using its topaz convert utility. We also provide a conversion utility at tools/coord_converter.py that may be helpful. Before training can proceed, the training coordinate files must be downscaled by the same factor used to downscale the micrographs. Assuming a downscaling factor of 6, and that your training data are available in demo_data/train_mrc/ and demo_data/train_coord/: . ```shell script topaz convert -s 6 -o demo_data/topaz_out/processed/particles.txt demo_data/train_coord/particles.txt . We then make new directories for our new model. ```shell script mkdir -p demo_data/topaz_out/saved_models . Topaz training can then be run as follows, where -n represents the approximate number of particles expected per micrograph. ```shell script topaz train -n 400 –num-workers 8 –train-images demo_data/topaz_out/processed/micrographs/ –train-targets demo_data/topaz_out/processed/particles.txt –save-prefix demo_data/topaz_out/saved_models/model -o demo_data/topaz_out/saved_models/model_training.txt . See `topaz train --help` for more detailed explanations of the arguments. This trained model can now be used to extract particles. In the following command, the `-r` parameter should be set to the radius of the particle you would like to pick. It is recommended that this be kept relatively small (as appropriate for your particle), as Topaz will not pick particles any closer than this to prevent multiple detections per particle. The `-x` parameter will upscale the resulting picks to the original micrograph, and should be the same as `-s` from `topaz preprocess`. The `-m` parameter should point to the last epoch of the model trained above. ```shell script topaz extract -r 14 -x 6 -m demo_data/topaz_out/saved_models/model_epoch10.sav \\ -o demo_data/topaz_out/predicted_particles_all_upsampled.txt \\ demo_data/topaz_out/processed/micrographs/*.mrc . Detection format conversion . Topaz provides a utility to convert the format of the extract output file, which can be used like so if needed (e.g. to convert from .txt to .star): . shell script topaz convert -o demo_data/topaz_out/predicted_particles_all_upsampled.star demo_data/topaz_out/predicted_particles_all_upsampled.txt . We also provide a script at tools/coord_converter.py that may be useful for coordinate file conversion. ",
    "url": "http://localhost:4000/topaz.html#installation",
    "relUrl": "/topaz.html#installation"
  },"31": {
    "doc": "EMPIAR",
    "title": "EMPIAR",
    "content": "# EMPIAR ## Introduction [EMPIAR](https://www.ebi.ac.uk/pdbe/emdb/empiar/) (the Electron Microscopy Public Image Archive) is an online database provided by EMBL-EBI. It contains several hundred (as of October 1, 2020) electron microscopy data sets, which can be downloaded for use in microscopy-based research. EMPIAR entries are organized by accession code, which are of the format `EMPIAR-#####`. For now, the `#####` portion starts with a `1` followed by zeros padding the data set number, e.g., `EMPIAR-10017`. Each EMPIAR entry also has a DOI number of the format `https://dx.doi.org/10.6019/EMPIAR-#####`, which should be used when citing data. This document describes how to transfer these data to a local machine or computing cluster for analysis. Since these data sets can be between hundreds of gigabytes and several terabytes in size, it is recommended to use a machine or cluster node with a dedicated high-bandwidth network connection for transfers. ## Method 1: Direct FTP Download This method performs a direct download operation using `wget`. It can therefore fail if the recipient machine experiences power loss or network timeout, or if the `wget` process or its containing shell session is interrupted. To protect transfers against these last two possibilities, we suggest using `screen` (manual page [here](https://linux.die.net/man/1/screen)) or `tmux` (manual page [here](https://linux.die.net/man/1/tmux)) if appropriate for your computing setup. See the linked manual pages or the [Protecting Transfers with `tmux`](#protecting-transfers-with-tmux) section for more information. To download a complete EMPIAR entry in the current working directory, run the following, replacing `#####` with the appropriate EMPIAR accession code. Note that the final `/` is important when using `wget`—otherwise, it will try to interpret `#####` as a file instead of a directory. ```shell script wget ftp://ftp.ebi.ac.uk/empiar/world_availability/#####/ ``` Note: the port is `22`, user is `anonymous` (or `Guest` on macOS), and password is left blank (although `wget` should identify these by default). ## Method 2: Globus For organizations that have computing clusters with a Globus endpoint, it is also possible to use the Globus service to transfer EMPIAR data much more rapidly, and without risk of interruption. These transfers can be managed from the Globus web interface, as follows, and do not rely on a shell session. 1. Towards the bottom of an EMPIAR entry page (e.g., https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10017/) click the `Browse Globus` link 2. Enter the name of your institution and follow login prompts 3. The File Manager should now be displayed; the Collection bar should say `Shared EMBL-EBI public endpoint` and the Path bar should say something like `/gridftp/empiar/world_availability/#####` 4. If the File Manager is not already in double-pane view, click `Transfer or Sync to...` in the menu at right (has an icon of two arrows pointing in opposite directions) 5. Click the search bar above the right-hand pane and enter the name of your organization's Globus endpoint 6. The right-hand pane should now populate with user-specific contents of the endpoint you specified; navigate to a suitable download location 7. Select files and/or directories to transfer in the left hand pane, then click `Start >` to download them to the destination specified in the right-hand Path bar Transfer progress is displayed in the Activity tab in the menu at left. The transfer will be scheduled by Globus, so the webpage can be closed after clicking `Start >`. ## Protecting Transfers with `tmux` The `tmux` utility can create persistent shell sessions, and is useful in protecting long file transfers to a remote machine against accidental disconnection of your SSH session. To create a new `tmux` session, use ```shell script tmux new -s my-session-name ``` To list active `tmux` sessions, use ```shell script tmux ls ``` To attach to an existing `tmux` session, use ```shell script tmux attach -t my-session-name-or-id ``` To detach from the current active `tmux` session (while keeping its processes running on the remote machine), use the following keyboard shortcut. Note that the `tmux` \"prefix\" combination (here, `Ctrl-b`) may be different on your system. ```shell script Ctrl-b + d ``` To kill a `tmux` session and all its processes (so that it does not continue consuming system resources after you're finished with it), use ```shell script tmux kill-session -t my-session-name-or-id ``` ",
    "url": "http://localhost:4000/empiar.html",
    "relUrl": "/empiar.html"
  }
}
